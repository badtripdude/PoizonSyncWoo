import asyncio
import re
from urllib.parse import urlparse

from loguru import logger


def extract_slug(url: str) -> str:
    """
    Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ slug Ğ¸Ğ· ÑÑÑ‹Ğ»ĞºĞ¸ thepoizon.ru/product/...
    """
    parsed = urlparse(url)
    # Ğ Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¿ÑƒÑ‚ÑŒ Ğ¸ Ğ±ĞµÑ€ĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ÑÑ Ñ‡Ğ°ÑÑ‚ÑŒ
    path_parts = parsed.path.strip("/").split("/")
    if "product" in path_parts:
        index = path_parts.index("product")
        if index + 1 < len(path_parts):
            return path_parts[index + 1]
    return ""


async def retry_async(
        func,
        *args,
        retries: int = 3,
        delay: float = 1.0,
        allowed_exceptions: tuple = (Exception,),
        **kwargs
):
    attempt = 0
    while attempt < retries:
        try:
            return await func(*args, **kwargs)
        except allowed_exceptions as e:
            attempt += 1
            logger.warning(f"[retry_async] ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğµ {func.__name__}: {e}. ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° {attempt}/{retries}")
            if attempt >= retries:
                logger.error(f"[retry_async] Ğ’ÑĞµ {retries} Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ¸ Ğ¸ÑÑ‡ĞµÑ€Ğ¿Ğ°Ğ½Ñ‹.")
                raise e
            await asyncio.sleep(delay)


def is_url(string):
    # ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğµ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ URL
    regex = re.compile(
        r'^(?:http|ftp)s?://'  # http:// Ğ¸Ğ»Ğ¸ https://
        r'(?:\S+(?::\S*)?@)?'  # Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ¼Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸ Ğ¿Ğ°Ñ€Ğ¾Ğ»ÑŒ
        r'(?:\S+\.)+[a-z]{2,}'  # Ğ´Ğ¾Ğ¼ĞµĞ½...
        r'(?::\d{2,5})?'  # Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ€Ñ‚
        r'(?:/\S*)?$',  # Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ URL
        re.IGNORECASE)

    return re.match(regex, string) is not None


def remove_chinese_characters(text: str) -> str:
    # ĞšĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹ Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‚ÑÑ Ğ² Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ°Ñ… Ğ®Ğ½Ğ¸ĞºĞ¾Ğ´Ğ°
    # chinese_pattern = re.compile(r'[\u4e00-\u9fff\u3400-\u4dbf\uf900-\ufaff]+')
    chinese_and_junk_pattern = re.compile(r'[\u4e00-\u9fff\u3400-\u4dbf\uf900-\ufaff\u3000-\u303F\uFF00-\uFFEF]+')
    return chinese_and_junk_pattern.sub('', text)


VARIANT_KEYS_TRANSLATION = {
    "å°ºç ": "size",
    "é¢œè‰²": "color",
    "æè´¨": "material",
    "é‹å¸®é«˜åº¦": "cut",
    "é€‚ç”¨å­£èŠ‚": "season",
    "é€‚ç”¨æ€§åˆ«": "gender",
    # ...
}

# ğŸŒ Ğ¡Ğ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ‚Ñ€Ğ°Ğ½ÑĞ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ (ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¹ â†’ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹)
VARIANT_VALUES_TRANSLATION = {
    "é»‘ç™½": "black & white",
    "ç™½é»‘": "white & black",
    "ç™½": "white",
    "é»‘": "black",
    "ç”·": "male",
    "å¥³": "female",
    "æ˜¥å­£": "spring",
    "ç§‹å­£": "autumn",
    "å¤å­£": "summer",
    "å†¬å­£": "winter",
    "ç™½é»‘ï¼ˆæè´¨å‡çº§æ¬¾ï¼‰": "white & black (upgraded material)",
    "ç²‰è‰²": "pink",
    # ...
}


def normalize_variants(raw_variants: dict[str, str]) -> dict[str, str]:
    """
    ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ğ¼Ğ¸ SKU Ğ¸Ğ· Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ° API
    Ğ² Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ ĞºĞ»ÑÑ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ.
    """
    normalized = {}
    for raw_key, raw_value in raw_variants.items():
        key = VARIANT_KEYS_TRANSLATION.get(raw_key.strip(), raw_key.strip())
        value = VARIANT_VALUES_TRANSLATION.get(raw_value.strip(), raw_value.strip())
        normalized[key] = value
    return normalized


def is_likely_kids_product(title: str) -> bool:
    """
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ True, ĞµÑĞ»Ğ¸ Ñ‚Ğ¾Ğ²Ğ°Ñ€ ÑĞºĞ¾Ñ€ĞµĞµ Ğ²ÑĞµĞ³Ğ¾ Ğ´ĞµÑ‚ÑĞºĞ°Ñ Ğ¾Ğ±ÑƒĞ²ÑŒ.
    """
    title = title.lower()

    kids_keywords = ["ç«¥é‹", "å°ç«¥", "å¤§ç«¥", "å„¿ç«¥", "å©´", "å­¦æ­¥é‹", "kids", "å„¿ç«¥æ¬¾", "child", "children", "baby",
                     "youth"]

    if any(kw in title for kw in kids_keywords):
        return True

    # if price < 400:
    #     return True

    # if sizes and all(float(s.replace("EU", "").strip()) < 36 for s in sizes if "EU" in s):
    #     return True

    return False
